document<-filenames[z]
word.count<-nrow(a)
temp.df<-data.frame(document, word.count, cause.vocab.per10K, pronoun.rate, coh.rate, trans.rate)
cause.df<-rbind(cause.df, temp.df)
}#end overall loop
View(cause.df)
z=1
a<-read.csv(filenames[z], sep="\t")
samp.v<-sample((max(a$sentenceID)-1), (max(a$sentenceID)-1), replace = F)
View(a)
trans.rate<-0
#for every sample sentence
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#if sentence has direct or indirect objects
if (length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))>0){
#for each one
for (j in 1:length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))){
#identify start point
obj.start<-which(sub$deprel == "iobj" | sub$deprel == "dobj")[j]
#find the verb anchor using headTokenID
if (grepl("VB", sub$pos[which(sub$tokenId == sub$headTokenId[obj.start])])){
#then check and see if it is stative, if so break
if (sub$supersense[which(sub$tokenId == sub$headTokenId[obj.start])] == "B-verb.stative"){
break
#if not then record phrase as transitive
} else {
trans.rate<-trans.rate+1
}
}#end conditional looking for stative/non-stative verb for each object
}#end loop for each object
}#end conditional for whether has objects
}
trans.rate
trans.rate<-trans.rate/length(samp.v)
trans.rate
length(samp.v)
t1<-trans.rate
trans.rate<-0
#for every sample sentence
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#if sentence has direct or indirect objects
if (length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))>0){
#for each one
for (j in 1:length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))){
#identify start point
obj.start<-which(sub$deprel == "iobj" | sub$deprel == "dobj")[j]
#find the verb anchor using headTokenID
if (grepl("VB", sub$pos[which(sub$tokenId == sub$headTokenId[obj.start])])){
#then check and see if it is stative, if so break
if (sub$supersense[which(sub$tokenId == sub$headTokenId[obj.start])] == "B-verb.stative"){
break
#if not then record phrase as transitive
} else {
trans.rate<-trans.rate+1
}
}#end conditional looking for stative/non-stative verb for each object
}#end loop for each object
}#end conditional for whether has objects
}
trans.rate
trans.rate/nrow(a)
View(cause.df)
View(test)
View(cause.df)
View(cause.df)
View(test)
View(cause.df)
View(test)
cause.df<-NULL
#for all docs
for (z in 1:length(filenames)){
print(z)
#ingest table
a<-read.csv(filenames[z], sep="\t")
#establish sample vector
#samp.v<-sample((max(a$sentenceID)-1), n, replace = T)
samp.v<-sample((max(a$sentenceID)-1), (max(a$sentenceID)-1), replace = F)
#### C0 #####
#explicit causal language
#causal language is indicated by a small set of vocabulary whose usage may be
#context dependent. Thus this script looks for keywords (or ngrams) but also takes
#grammatical context into account to avoid false positives. An example is the word
#"so" which can mean "so he went home" or "so big". The first is arguably part of
#causal reasoning, the second not.
#set the causal vocabulary rate to 0
cause.rate<-0
### Always causal words ###
#create list of always causal ngrams
c.always.1<-c("thus","therefore","despite","consequently","because","although",
"depended", "dependent", "depends", "depending", "depend", "dependency")
c.always.2<-c("so that", "due to")
c.always.3<-c("for that reason", "to that end", "as a result", "so as to", "in order to")
#calculate # of 1gram matches
cause.rate<-cause.rate+length(which(as.character(a$lemma) %in% c.always.1))
#generate vector of bigrams
bi<-cbind(as.character(a$lemma)[-length(a$lemma)], as.character(a$lemma)[-1])
bi<-paste(bi[,1], bi[,2], sep=" ")
#calculate number of bigram matches
cause.rate<-cause.rate+length(which(bi %in% c.always.2))
#generate vector of trigrams
tri<-cbind(as.character(a$lemma)[-length(a$lemma)], as.character(a$lemma)[-1], as.character(a$lemma)[3:(length(a$lemma)+1)])
tri<-paste(tri[,1], tri[,2], tri[,3],sep=" ")
#calculate number of trigram matches
cause.rate<-cause.rate+length(which(tri %in% c.always.3))
### Simple Causal Words in Context ###
though<-a[a$lemma == "though" & a$pos == "IN",]
so<-a[a$lemma == "so" & a$pos == "IN",]
effect<-a[a$lemma == "effect" & grepl("NN", a$pos) & a$deprel != "pobj",]
cause.rate<-cause.rate+nrow(though)+nrow(so)+nrow(effect)
### Harder causal words in context ###
#by
by<-a[a$lemma == "by",]
#only keep instances where the reference (headTokenID) is backwards
by<-by[by$headTokenId < by$tokenId,]
#since
since<-a[a$lemma == "since",]
#only keep instances where reference is forwards and deprel is not a preposition
since<-since[since$headTokenId > since$tokenId & since$deprel != "prep",]
cause.rate<-cause.rate+nrow(by)+nrow(since)
#calculate final score
cause.vocab.per10K<-(cause.rate/nrow(a))*10000
#### C1 ######
#pronoun rate
#higher levels of pronoun create more referential ambiguity
pronoun.rate<-nrow(a[grep("PRP", a$pos),])/nrow(a)
##### C2 ######
#entity cohesion rate
#is an entity passed from sentence S to S+1
#higher cohesion = greater implicit causality
#set rate to 0
coh.rate<-0
#for all sentences in sample
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#subset by next sentence
sub2<-a[a$sentenceID == (samp.v[i]+1),]
#is an entity passed from sentence S to sentence S+1?
#represent each sentence as a vector of entities (nouns + pronouns)
s1<-append(as.character(sub$lemma[grep("NN", sub$pos)]), as.character(sub$lemma[grep("PRP", sub$pos)]))
s2<-append(as.character(sub2$lemma[grep("NN", sub2$pos)]), as.character(sub2$lemma[grep("PRP", sub2$pos)]))
#if there is an(y) intersection, then adjust cohesion score by 1
if (length(which(s1 %in% s2))>0){
coh.rate<-coh.rate+1
}
} #end C2
coh.rate<-coh.rate/length(samp.v)
#C3
#realis rate
#lower levels of modality and hypothetical events = greater implicity causality
#C4
#transitivity rate
#transitivity = non-stative verbs that take indirect or direct objects
#greater levels of of transitivity = greater implicit causality
#establish the transitivity rate as 0
trans.rate<-0
#for every sample sentence
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#if sentence has direct or indirect objects
if (length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))>0){
#for each one
for (j in 1:length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))){
#identify start point
obj.start<-which(sub$deprel == "iobj" | sub$deprel == "dobj")[j]
#find the verb anchor using headTokenID
if (grepl("VB", sub$pos[which(sub$tokenId == sub$headTokenId[obj.start])])){
#then check and see if it is stative, if so break
if (sub$supersense[which(sub$tokenId == sub$headTokenId[obj.start])] == "B-verb.stative"){
break
#if not then record phrase as transitive
} else {
trans.rate<-trans.rate+1
}
}#end conditional looking for stative/non-stative verb for each object
}#end loop for each object
}#end conditional for whether has objects
}#end C4
#trans.rate<-trans.rate/nrow(a)
trans.rate<-trans.rate/length(samp.v)
#C5 realis transitivity
#condition only on realis events with dobj and iobj
#store results in data frame
document<-filenames[z]
word.count<-nrow(a)
sentence.count<-length(samp.v)
sentence.length<-word.count/sentence.count
temp.df<-data.frame(document, word.count, sentence.count, sentence.length, cause.vocab.per10K, pronoun.rate, coh.rate, trans.rate)
cause.df<-rbind(cause.df, temp.df)
}
View(cause.df)
View(a)
word.length<-nchar(a$originalWord[a$deprel != "punct"])
a$originalWord[a$deprel != "punct"]
as.character(a$originalWord[a$deprel != "punct"])
nchar(as.character(a$originalWord[a$deprel != "punct"]))
word.length<-mean(nchar(as.character(a$originalWord[a$deprel != "punct"])))
word.length
cause.df<-NULL
#for all docs
for (z in 1:length(filenames)){
print(z)
#ingest table
a<-read.csv(filenames[z], sep="\t")
#establish sample vector
#samp.v<-sample((max(a$sentenceID)-1), n, replace = T)
samp.v<-sample((max(a$sentenceID)-1), (max(a$sentenceID)-1), replace = F)
#### C0 #####
#explicit causal language
#causal language is indicated by a small set of vocabulary whose usage may be
#context dependent. Thus this script looks for keywords (or ngrams) but also takes
#grammatical context into account to avoid false positives. An example is the word
#"so" which can mean "so he went home" or "so big". The first is arguably part of
#causal reasoning, the second not.
#set the causal vocabulary rate to 0
cause.rate<-0
### Always causal words ###
#create list of always causal ngrams
c.always.1<-c("thus","therefore","despite","consequently","because","although",
"depended", "dependent", "depends", "depending", "depend", "dependency")
c.always.2<-c("so that", "due to")
c.always.3<-c("for that reason", "to that end", "as a result", "so as to", "in order to")
#calculate # of 1gram matches
cause.rate<-cause.rate+length(which(as.character(a$lemma) %in% c.always.1))
#generate vector of bigrams
bi<-cbind(as.character(a$lemma)[-length(a$lemma)], as.character(a$lemma)[-1])
bi<-paste(bi[,1], bi[,2], sep=" ")
#calculate number of bigram matches
cause.rate<-cause.rate+length(which(bi %in% c.always.2))
#generate vector of trigrams
tri<-cbind(as.character(a$lemma)[-length(a$lemma)], as.character(a$lemma)[-1], as.character(a$lemma)[3:(length(a$lemma)+1)])
tri<-paste(tri[,1], tri[,2], tri[,3],sep=" ")
#calculate number of trigram matches
cause.rate<-cause.rate+length(which(tri %in% c.always.3))
### Simple Causal Words in Context ###
though<-a[a$lemma == "though" & a$pos == "IN",]
so<-a[a$lemma == "so" & a$pos == "IN",]
effect<-a[a$lemma == "effect" & grepl("NN", a$pos) & a$deprel != "pobj",]
cause.rate<-cause.rate+nrow(though)+nrow(so)+nrow(effect)
### Harder causal words in context ###
#by
by<-a[a$lemma == "by",]
#only keep instances where the reference (headTokenID) is backwards
by<-by[by$headTokenId < by$tokenId,]
#since
since<-a[a$lemma == "since",]
#only keep instances where reference is forwards and deprel is not a preposition
since<-since[since$headTokenId > since$tokenId & since$deprel != "prep",]
cause.rate<-cause.rate+nrow(by)+nrow(since)
#calculate final score
cause.vocab.per10K<-(cause.rate/nrow(a))*10000
#### C1 ######
#pronoun rate
#higher levels of pronoun create more referential ambiguity
pronoun.rate<-nrow(a[grep("PRP", a$pos),])/nrow(a)
##### C2 ######
#entity cohesion rate
#is an entity passed from sentence S to S+1
#higher cohesion = greater implicit causality
#set rate to 0
coh.rate<-0
#for all sentences in sample
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#subset by next sentence
sub2<-a[a$sentenceID == (samp.v[i]+1),]
#is an entity passed from sentence S to sentence S+1?
#represent each sentence as a vector of entities (nouns + pronouns)
s1<-append(as.character(sub$lemma[grep("NN", sub$pos)]), as.character(sub$lemma[grep("PRP", sub$pos)]))
s2<-append(as.character(sub2$lemma[grep("NN", sub2$pos)]), as.character(sub2$lemma[grep("PRP", sub2$pos)]))
#if there is an(y) intersection, then adjust cohesion score by 1
if (length(which(s1 %in% s2))>0){
coh.rate<-coh.rate+1
}
} #end C2
coh.rate<-coh.rate/length(samp.v)
#C3
#realis rate
#lower levels of modality and hypothetical events = greater implicity causality
#C4
#transitivity rate
#transitivity = non-stative verbs that take indirect or direct objects
#greater levels of of transitivity = greater implicit causality
#establish the transitivity rate as 0
trans.rate<-0
#for every sample sentence
for (i in 1:length(samp.v)){
#subset by sentence
sub<-a[a$sentenceID == samp.v[i],]
#if sentence has direct or indirect objects
if (length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))>0){
#for each one
for (j in 1:length(which(sub$deprel == "iobj" | sub$deprel == "dobj"))){
#identify start point
obj.start<-which(sub$deprel == "iobj" | sub$deprel == "dobj")[j]
#find the verb anchor using headTokenID
if (grepl("VB", sub$pos[which(sub$tokenId == sub$headTokenId[obj.start])])){
#then check and see if it is stative, if so break
if (sub$supersense[which(sub$tokenId == sub$headTokenId[obj.start])] == "B-verb.stative"){
break
#if not then record phrase as transitive
} else {
trans.rate<-trans.rate+1
}
}#end conditional looking for stative/non-stative verb for each object
}#end loop for each object
}#end conditional for whether has objects
}#end C4
trans.rate<-trans.rate/nrow(a)
#C5 realis transitivity
#condition only on realis events with dobj and iobj
#basic diagnostic statistics
word.count<-nrow(a)
sentence.count<-length(samp.v)
sentence.length<-word.count/sentence.count
word.length<-mean(nchar(as.character(a$originalWord[a$deprel != "punct"])))
#store results in data frame
document<-filenames[z]
temp.df<-data.frame(document, word.count, sentence.count, sentence.length, cause.vocab.per10K, pronoun.rate, coh.rate, trans.rate)
cause.df<-rbind(cause.df, temp.df)
}#end overall loop
View(cause.df)
vec<-vector()
for (z in 1:length(filenames)){
print(z)
#ingest table
a<-read.csv(filenames[z], sep="\t")
word.length<-mean(nchar(as.character(a$originalWord[a$deprel != "punct"])))
vec<-append(vec, word.length)
}
vec
View(cause.df)
filenames
cause.df$word.length<-vec
library(topicmodels)
library(entropy)
setwd("~/Websites/git_piperandrew/handbook")
filenames<-list.files("topicmodel_samples")
setwd("~/Websites/git_piperandrew/handbook/topicmodel_samples")
#load primary model
twp<-read.csv("model_original.csv")
setwd("~/Websites/git_piperandrew/handbook")
filenames<-list.files("topicmodel_samples")
setwd("~/Websites/git_piperandrew/handbook/topicmodel_samples")
twp<-read.csv("model_original.csv")
View(twp)
View(twp)
twp<-twp[,-1]
twp<-t(twp)
colnames(twp)<-seq(1,ncol(twp))
View(twp)
stable.df<-NULL
i=1
nrow(topic_word_probs)
ncol(twp)
ncol(twp)
sub1<-twp[,i]
(length(filenames)-1)
test.t<-NULL
j=1
comp<-read.csv(filenames[j])
comp<-comp[,-1]
comp<-t(comp)
colnames(comp)<-seq(1,ncol(comp))
kld.v<-vector()
for (k in 1:ncol(comp)){
kld.v[k]<-KL.plugin(sub1, comp[,k])
}
top.t<-which(kld.v == min(kld.v))
model<-j
model
kld.v
View(twp)
View(comp)
top.t
model<-j
kld.score<-kld.v[which(kld.v == min(kld.v))]
kld.score
temp.df<-data.frame(model, top.t, kld.score)
test.t<-rbind(test.t, temp.df)
View(test.t)
test.t<-NULL
#run through all but final model, which is your original
for (j in 1:(length(filenames)-1)){
#load next model
comp<-read.csv(filenames[j])
#clean
comp<-comp[,-1]
comp<-t(comp)
colnames(comp)<-seq(1,ncol(comp))
#comp should now mirror twp
#go through every topic in comp to find most similar topic in twp
#calculate KLD for every topic pair with the ith topic from primary model
kld.v<-vector()
for (k in 1:ncol(comp)){
kld.v[k]<-KL.plugin(sub1, comp[,k])
}
#find minimum value, i.e. most similar topic
top.t<-which(kld.v == min(kld.v))
#which model
model<-j
#what was the divergence?
kld.score<-kld.v[which(kld.v == min(kld.v))]
#create data frame
temp.df<-data.frame(model, top.t, kld.score)
test.t<-rbind(test.t, temp.df)
}
test.t
mean.kld<-mean(test.t$kld.score)
mean.kld
sd.kld<-sd(test.t$kld.score)
stable.df<-NULL
#run for every topic
for (i in 1:ncol(twp)){
print(i)
#subset by ith topic
sub1<-twp[,i]
#go through each model and find most similar topic
test.t<-NULL
#run through all but final model, which is your original
for (j in 1:(length(filenames)-1)){
#load next model
comp<-read.csv(filenames[j])
#clean
comp<-comp[,-1]
comp<-t(comp)
colnames(comp)<-seq(1,ncol(comp))
#comp should now mirror twp
#go through every topic in comp to find most similar topic in twp
#calculate KLD for every topic pair with the ith topic from primary model
kld.v<-vector()
for (k in 1:ncol(comp)){
kld.v[k]<-KL.plugin(sub1, comp[,k])
}
#find minimum value, i.e. most similar topic
top.t<-which(kld.v == min(kld.v))
#which model
model<-j
#what was the divergence?
kld.score<-kld.v[which(kld.v == min(kld.v))]
#create data frame
temp.df<-data.frame(model, top.t, kld.score)
test.t<-rbind(test.t, temp.df)
}
#calculate mean and sd for the ith topic compared to best topic of all other models
mean.kld<-mean(test.t$kld.score)
sd.kld<-sd(test.t$kld.score)
topic<-i
temp.df<-data.frame(topic, mean.kld, sd.kld)
stable.df<-rbind(stable.df, temp.df)
}
View(stable.df)
View(stable.df)
load("~/Websites/git_piperandrew/handbook/00_Fish_TopicModel_Novel150.RData")
probabilities<-posterior(topicmodel)
term_dis<-terms(topicmodel, 20)
View(term_dis)
topic_word_probs<-as.data.frame(probabilities$terms)
View(topic_word_probs)
View(twp)
topic_word_probs<-t(topic_word_probs)
View(topic_word_probs)
twp<-topic_word_probs
test<-stable.df
stable.df<-NULL
for (i in 1:ncol(twp)){
print(i)
#subset by ith topic
sub1<-twp[,i]
#go through each model and find most similar topic
test.t<-NULL
#run through all but final model, which is your original
for (j in 1:(length(filenames)-1)){
#load next model
comp<-read.csv(filenames[j])
#clean
comp<-comp[,-1]
comp<-t(comp)
colnames(comp)<-seq(1,ncol(comp))
#comp should now mirror twp
#go through every topic in comp to find most similar topic in twp
#calculate KLD for every topic pair with the ith topic from primary model
kld.v<-vector()
for (k in 1:ncol(comp)){
kld.v[k]<-KL.plugin(sub1, comp[,k])
}
#find minimum value, i.e. most similar topic
top.t<-which(kld.v == min(kld.v))
#which model
model<-j
#what was the divergence?
kld.score<-kld.v[which(kld.v == min(kld.v))]
#create data frame
temp.df<-data.frame(model, top.t, kld.score)
test.t<-rbind(test.t, temp.df)
}
#calculate mean and sd for the ith topic compared to best topic of all other models
mean.kld<-mean(test.t$kld.score)
sd.kld<-sd(test.t$kld.score)
topic<-i
temp.df<-data.frame(topic, mean.kld, sd.kld)
stable.df<-rbind(stable.df, temp.df)
}
View(test)
View(test)
View(stable.df)
