temp.df<-data.frame(topic.names, topic.probs, time.window)
topic.co.df<-rbind(topic.co.df, temp.df)
}
#then take average of each topic
topic.mean<-tapply(topic.co.df$topic.probs, topic.co.df$topic.names, mean)
#calculate the % of documents that these topics occur in
topic.count<-tapply(as.character(topic.co.df$topic.names), topic.co.df$topic.names, table)
topic.per<-topic.count/nrow(sub3)
temp.df<-data.frame(time.window, topic.mean, topic.count, topic.per)
topic.number<-row.names(temp.df)
temp.df$topic.number<-topic.number
topic.co.final<-rbind(topic.co.final, temp.df)
}
}
plot(sort(tapply(topic.co.final$topic.count, as.factor(topic.co.final$topic.number), sum), decreasing = T)[2:nlevels(as.factor(topic.co.final$topic.number))])
View(topic.co.final)
lead.tops<-sort(tapply(topic.co.final$topic.count, as.factor(topic.co.final$topic.number), sum), decreasing = T)[2:4]
topic.sub<-topic.co.final[which(topic.co.final$topic.number %in% names(lead.tops)),]
topic.sub$topic.number<-as.factor(topic.sub$topic.number)
topic.sub$year<-as.numeric(substr(topic.sub$time.window,1,4))
View(topic.sub)
lead.tops
lead.tops<-sort(tapply(topic.co.final$topic.count, as.factor(topic.co.final$topic.number), sum), decreasing = T)[2:5]
topic.sub<-topic.co.final[which(topic.co.final$topic.number %in% names(lead.tops)),]
topic.sub$topic.number<-as.factor(topic.sub$topic.number)
topic.sub$year<-as.numeric(substr(topic.sub$time.window,1,4))
lead.tops
topic.sub$topic.number<-as.character(topic.sub$topic.number)
View(term_dis)
View(term_dis)
View(term_dis)
View(topic.co.final)
time.w<-seq(as.numeric(levels(factor(j1$year)))[i],levels(factor(j1$year))[i+4])
time.w
sub1<-j1[which(j1$year %in% time.w),]
sub2<-top.df[which(row.names(top.df) %in% as.character(sub1$file)),]
View(sub2)
j1[j1$file == "10.1086%2F491682",]
View(term_dis)
View(term_dis)
topic.sub$topic.number[topic.sub$topic.number == "21"]<-"cultural studies"
topic.sub$topic.number[topic.sub$topic.number == "1"]<-"education"
topic.sub$topic.number[topic.sub$topic.number == "43"]<-"sexuality"
topic.sub$topic.number[topic.sub$topic.number == "30"]<-"social research"
ggplot(topic.sub, aes(x=year, y=topic.per*100, color=topic.number)) +
theme_bw() +
geom_line() +
scale_color_discrete(name="Topics") +
#guides(fill=guide_legend(title="Topics"))+
labs(x="Year", y="% Articles", title="Gender Studies", subtitle = "Percentage of documents with co-occurring topics, 1950-2010 (N=2,670)", caption="Source: JSTOR Data for Research")
library(ggplot2)
ggplot(topic.sub, aes(x=year, y=topic.per*100, color=topic.number)) +
theme_bw() +
geom_line() +
scale_color_discrete(name="Topics") +
#guides(fill=guide_legend(title="Topics"))+
labs(x="Year", y="% Articles", title="Gender Studies", subtitle = "Percentage of documents with co-occurring topics, 1950-2010 (N=2,670)", caption="Source: JSTOR Data for Research")
ggplot(topic.sub, aes(x=year, y=topic.count, color=topic.number)) +
theme_bw() +
geom_line() +
scale_color_discrete(name="Topics") +
#guides(fill=guide_legend(title="Topics"))+
labs(x="Year", y="No. Articles", title="Cultural Studies", subtitle = "Number of documents with co-occurring topics, 1950-2010 (N=2,670)", caption="Source: JSTOR Data for Research")
View(j1)
test<-j1[j1$journal_title == "Signs",]
View(test)
test<-j1[j1$journal_title == "Feminist Studies",]
View(test)
log2(.005)
-log2(.005)
-log2(.0005)
log2(.005/.00005)
log2(.005/.00005)*.00005
log2(.005/.000005)*.000005
v<-sample(6:20,50,replace=T)
v
plot(v)
plot(v, type="line")
plot(v, type="line", color="blue", xlab="Sentences over narrative time", ylab="# words")
plot(v, type="line", col="blue", xlab="Sentences over narrative time", ylab="# words")
plot(v, type="line", col="blue", xlab="Sentences over narrative time", ylab="# words", ylim=c(0,25))
lo <- loess(y~x)
plot(x,y)
lines(predict(lo), col='red', lwd=2)
v2<-1:50
lo<-loess(v~v2)
plot(v2,v)
lines(predict(lo), col='red', lwd=2)
install.packages("tm")
library("tm")
setwd("~/Data")
corpus1 <- VCorpus(DirSource("txtlab_Novel150_English", encoding = "UTF-8"), readerControl=list(language="English"))
#Inspect your data
head(corpus)
#Inspect your data
head(corpus1)
#Inspect your data
inspect(corpus1[1])
#Inspect your data (see the actual text)
inspect(corpus1[[1]])
#Inspect your data (see the actual text)
head(inspect(corpus1[[1]]))
gc()
#Inspect your data (see the actual text)
inspect(corpus1[[1]])[1:2]
gc()
#Inspect your data (see the actual text)
inspect(corpus1[[1]])
head(strwrap(corpus1))
gc()
head(strwrap(corpus1[[1]]))
strwrap(corpus1[[1]])[1:10]
inspect(corpus1[3]) #the number in brackets refers to the document number
strwrap(corpus1[[3]])[1:10]
#Inspect your data (see the actual text)
strwrap(corpus1[[3]])[1:20]
#Inspect your data (see the actual text)
strwrap(corpus1[[3]])[1:15]
length(strwrap(corpus1[[3]]))
inspect(corpus1[26]) #the number in brackets refers to the document number
strwrap(corpus1[[26]])[1:15]
setwd("~/Data")
#Read in your corpus
corpus1 <- VCorpus(DirSource("txtlab_Novel150_English", encoding = "UTF-8"), readerControl=list(language="English"))
inspect(corpus1[26]) #the number in brackets refers to the document number
strwrap(corpus1[[26]])[1:15]
strwrap(corpus1[[26]])[1:10]
#Inspect your data (see a portion of the actual text)
strwrap(corpus1[[26]])[1:5]
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
gc()
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
strwrap(corpus1[[26]])[1:5]
install.packages("textstem")
corpus1 <- tm_map(corpus1, content_transformer(lemmatize_words))
library(textstem)
?install.koRpus.lang()
available.koRpus.lang(repos = "https://undocumeantit.github.io/repos/l10n/")
install.koRpus.lang(c("de", "fr")
)
corpus1 <- tm_map(corpus1, content_transformer(lemmatize_words))
strwrap(corpus1[[26]])[1:5]
#inspect
strwrap(corpus1[[26]])[1:10]
#stem words
#corpus1 <- tm_map(corpus1, stemDocument, language = "english")
corpus1 <- tm_map(corpus1, lemmatize_words)
strwrap(corpus1[[26]])[1:10]
#stem words
#corpus1 <- tm_map(corpus1, stemDocument, language = "english")
corpus1 <- tm_map(corpus1, lemmatize_strings)
warnings()
corpus1 <- tm_map(corpus1, PlainTextDocument)
strwrap(corpus1[[26]])[1:10]
#inspect
strwrap(corpus1[[26]])[1:5]
corpus1 <- VCorpus(DirSource("txtlab_Novel150_English", encoding = "UTF-8"), readerControl=list(language="English"))
corpus1 <- tm_map(corpus1, content_transformer(stripWhitespace))
#make all lowercase
corpus1 <- tm_map(corpus1, content_transformer(tolower))
#remove numbers
corpus1 <- tm_map(corpus1, content_transformer(removeNumbers))
#remove punctuation
corpus1 <- tm_map(corpus1, content_transformer(removePunctuation))
corpus1.dtm<-DocumentTermMatrix(corpus1, control=list(wordLengths=c(1,Inf)))
install.packages("slam")
library(slam)
row_sums(corpus1.dtm)
#How many documents do you have?
nrow(corpus1.dtm)
#How many word types?
ncol(corpus1.dtm)
sum(corpus1.dtm)
row_sums(corpus1.dtm)
#what is the distribution of word counts across your data?
hist(row_sums(corpus1.dtm))
options(scipen=9)
#what is the distribution of word counts across your data?
hist(row_sums(corpus1.dtm))
#what is the average length of a novel?
mean(row_sums(corpus1.dtm))
median(row_sums(corpus1.dtm))
summary(row_sums(corpus1.dtm))
wc<-row_sums(corpus1.dtm)
write.csv(wc, file="wc.csv")
#How can you find out which is the longest novel?
max(row_sums(corpus1.dtm))
#How can you find out which is the longest novel?
which.max(row_sums(corpus1.dtm))
row_sums(corpus1.dtm)[13]
dtm.scaled<-corpus1.dtm/row_sums(corpus1.dtm)
dtm.scaled[1]
dtm.scaled[[1]]
dtm.scaled$v[1:10]
row_sums(corpus1.dtm)[1]
1/36471
#Method 3: log
#here we just take the log value of the raw counts
#this makes the nonlinear relationship between word counts more linear
dtm.log<-log(corpus1.dtm)
#Use this when you want to calculate something specific on each document
f.names<-list.files(path="txtlab_Novel150_English")
f.names
#Use this when you want to calculate something specific on each document
f.names<-list.files("txtlab_Novel150_English")
f.names
#get list of files in your directory
f.names<-list.files("Data/txtlab_Novel150_English")
f.names
#get list of files in your directory
f.names<-list.files("~Data/txtlab_Novel150_English")
f.names
#get list of files in your directory
f.names<-list.files("/Data/txtlab_Novel150_English")
f.names
f.names<-list.files("txtlab_Novel150_English")
f.names
setwd("~/Data/txtlab_Novel150_English")
#get list of files in your directory
f.names<-list.files("~/Data/txtlab_Novel150_English")
f.names
f.names<-list.files("txtlab_Novel150_English")
f.names
setwd("~/Data")
f.names<-list.files("txtlab_Novel150_English")
f.names
setwd("~/Data/txtlab_Novel150_English")
res <- sapply(f.names, file.size)
res
work<-scan(f.names[1], what="character", quote="")
work
#remove punctuation
work<- gsub("\\d", "", work)
work
#remove punctuation
work<-gsub("\\W", "", work)
work
work<- tolower(work)
work<- tolower(work)
work<-work[work != ""]
work
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<- gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<- tolower(work)
#remove blanks
work<-work[work != ""]
}
text.prep(f.names[1])
work
text.prep(f.names[2])
work
f.names[2]
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<- gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<- tolower(work)
#remove blanks
work<-work[work != ""]
return(work)
}
f.names[2]
text.prep(f.names[2])
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<- gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<- tolower(work)
#remove blanks
work<-work[work != ""]
}
text.prep(f.names[1])
work
text.prep(f.names[2])
work
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<-gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<-tolower(work)
#remove blanks
work<-work[work != ""]
}
text.prep(f.names[2])
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<-gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<-tolower(work)
#remove blanks
work<-work[work != ""]
work<-return(work)
}
text.prep(f.names[2])
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="")
#remove numbers
work<-gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<-tolower(work)
#remove blanks
work<-work[work != ""]
return(length(work))
}
text.prep(f.names[2])
text.prep(f.names[1])
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="", quiet=T)
#remove numbers
work<-gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<-tolower(work)
#remove blanks
work<-work[work != ""]
return(length(work))
}
text.prep(f.names[1])
text.prep(f.names[2])
v<-text.prep(f.names[2])
v
text.prep<-function(x){
#first scan in the document
work<-scan(x, what="character", quote="", quiet=T)
#remove numbers
work<-gsub("\\d", "", work)
#remove punctuation
work<-gsub("\\W", "", work)
#make all lowercase
work<-tolower(work)
#remove blanks
work<-work[work != ""]
}
v<-text.prep(f.names[2])
v
v<-text.prep(f.names[1]) #this runs the function on the first document
v[1:10]
work.v<-text.prep(f.names[1]) #this runs the function on the first document
work.v[1:10] #this shows you the first ten words of the first document -- everything should be lowercase and have no punctuation
ttr<-function(work.v, winsize){
#first locate a random starting point
#take one random number from the overall length of the work (not including anything closer to the end than your sample will be, i.e. going past the end)
beg<-sample(1:(length(work.v)-winsize),1)
#extract that segment as a new vector
test<-filename[beg:(beg+winsize)]
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-length(unique(test))/length(test)
}
ttr(work.v, 500)
ttr<-function(work.v, winsize){
#first locate a random starting point
#take one random number from the overall length of the work (not including anything closer to the end than your sample will be, i.e. going past the end)
beg<-sample(1:(length(work.v)-winsize),1)
#extract that segment as a new vector
test<-work.v[beg:(beg+winsize)]
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-length(unique(test))/length(test)
}
ttr(work.v, 500)
t<-ttr(work.v, 500)
t
work.v
beg<-sample(1:(length(work.v)-winsize),1)
winsize = 500
beg<-sample(1:(length(work.v)-winsize),1)
beg
test<-work.v[beg:(beg+winsize)]
test
test<-work.v[beg:(beg+(winsize-1))]
test
ttr.sample<-length(unique(test))/length(test)
ttr.sample
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-rep(length(unique(test))/length(test), 100)
ttr.sample
ttr.sample<-length(unique(test))/length(test)
ttr.sample
t<-rep(ttr(work.v, 500), 100)
t
ttr<-function(work.v, winsize){
#first locate a random starting point
#take one random number from the overall length of the work (not including anything closer to the end than your sample will be, i.e. going past the end)
beg<-sample(1:(length(work.v)-winsize),1)
#extract that segment as a new vector
test<-work.v[beg:(beg+(winsize-1))]
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-length(unique(test))/length(test)
}
t<-rep(ttr(work.v, 500), 100)
t
ttr(work.v, 500)
ttr<-function(work.v, winsize){
#first locate a random starting point
#take one random number from the overall length of the work (not including anything closer to the end than your sample will be, i.e. going past the end)
beg<-sample(1:(length(work.v)-winsize),1)
#extract that segment as a new vector
test<-work.v[beg:(beg+(winsize-1))]
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-length(unique(test))/length(test)
return(ttr.sample)
}
t<-rep(ttr(work.v, 500), 100)
t
ttr(work.v, 500)
ttr(work.v, 500)
t<-for (i in 1:100){ttr.sample(work.v, 500)}
t<-for (i in 1:100){ttr(work.v, 500)}
t
work.v
beg
beg<-sample(1:(length(work.v)-winsize),1)
beg
ttr<-function(work.v, winsize){
#first locate a random starting point
#take one random number from the overall length of the work (not including anything closer to the end than your sample will be, i.e. going past the end)
beg<-sample(1:(length(work.v)-winsize),1)
#extract that segment as a new vector
test<-work.v[beg:(beg+(winsize-1))]
#calculate TTR
#TTR = unique words / total words (i.e. types divided by tokens)
ttr.sample<-length(unique(test))/length(test)
return(ttr.sample)
}
ttr(work.v, 500)
ttr(work.v, 500)
ttr(work.v, 500)
ttr(work.v, 500)
replicate(100,ttr(work.v, winsize))
replicate(100,ttr(work.v, 500))
#you can take the mean to get the avg ttr for this text
mean(replicate(100,ttr(work.v, 500)))
#to run the function you
ttr(work.v, 500)
res <- sapply(f.names, text.prep)
res <- sapply(f.names, text.prep)
res
length(f.names)
work.v<-text.prep(f.names[i])
work.v
i
i=1
work.v<-text.prep(f.names[i])
work.v
#calculate TTR
ttr.v<-replicate(100,ttr(work.v, 500))
ttr.v
ttr.v<-mean(replicate(100,ttr(work.v, 500)))
ttr.v
temp.df<-data.frame(filename, ttr.mean)
filename<-f.names[i]
temp.df<-data.frame(filename, ttr.mean)
ttr.mean<-mean(replicate(100,ttr(work.v, 500)))
filename<-f.names[i]
temp.df<-data.frame(filename, ttr.mean)
temp.df
ttr.df<-NULL
#create a loop that is as long as the number of documents
for (i in 1:length(f.names)){
#see how fast things are going
print(i)
#ingest and clean each text
work.v<-text.prep(f.names[i])
#calculate the mean value of a vector of 100 TTR values
ttr.mean<-mean(replicate(100,ttr(work.v, 500)))
#store everything in a table
filename<-f.names[i]
temp.df<-data.frame(filename, ttr.mean)
ttr.df<-rbind(ttr.df, temp.df)
}
View(ttr.df)
View(ttr.df)
View(ttr.df)
BigramTokenizer2 <- function(x)unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
BigramTokenizer12 <- function(x)unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
BigramTokenizer2 <- function(x)unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm.bigram <- DocumentTermMatrix(model, control=list(tokenize = BigramTokenizer2, wordLengths=c(1,Inf)))
BigramTokenizer2 <- function(x)unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm.bigram <- DocumentTermMatrix(corpus1, control=list(tokenize = BigramTokenizer2, wordLengths=c(1,Inf)))
